# -*- coding: utf-8 -*-
"""Predicting Sepsis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QqCHtLMTFLvmP3L5NosaCN_fH5NhSkfc

# The Impact of Data Transformations on Sepsis Prediction
The purpose of this project was to determine if transformations have an impact on sepsis model perfomance. There are known issues with the predictive performance of the commonly used Epic sepsis prediction model, so I wanted to understand why such a stark variance in predictive power could occur. It's my hypothesis that the way the data is ingested by the model, coupled with differentiation in data collection methods that resulted in the reduced predictive performance of the model(Wong et al.). This project places a heavy emphasis on transformative approaches, utilizing a keras based machine learning binary classification algorithm as our central benchmarking tool. 

There have been other attempts to classify binary outcomes with this dataset, my goal was to understand if we could improve upon them with effective feature engineering/imputation techniques. If this project demonstrates improvements between techniques it lays the groundwork for improving the data ingestion pipeline for the epic model.
"""

from google.colab import files



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from numpy import mean
from numpy import std
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance
from sklearn.metrics import accuracy_score


from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

pd.set_option("display.max_rows", None, "display.max_columns", None)

files.upload()

files.upload()

"""# Initial EDA and Data Overview
The data I gathered came from a kaggle dataset posted online (that was later redacted, presumably as a result of PHI violations). It consists of 39 seperate variables, including a sepsis binary classification. The amount of data present per variable varied significantly, but served as an effective base example of common EHR data. Below is an overview of the data present in the dataset. 

1. HR - Heart rate (beats per minute)
2. O2Sat - Pulse oximetry (%);
3. Temp - Temperature (Deg C)
4. SBP - Systolic BP (mm Hg)
5. MAP - Mean arterial pressure (mm Hg)
6. DBP - Diastolic BP (mm Hg)
7. Resp - Respiration rate (breaths per minute)
8. EtCO2 - End tidal carbon dioxide (mm Hg)
9. BaseExcess - Measure of excess bicarbonate (mmol/L)
10. HCO3 - Bicarbonate (mmol/L)
11. FiO2 - Fraction of inspired oxygen (%)
12. pH - N/A
13. PaCO2 - Partial pressure of carbon dioxide from arterial blood (mm Hg)
14. SaO2 - Oxygen saturation from arterial blood (%)
15. AST - Aspartate transaminase (IU/L)
16. BUN - Blood urea nitrogen (mg/dL)
17. Alkalinephos - Alkaline phosphatase (IU/L)
18. Calcium - (mg/dL)
19. Chloride - (mmol/L)
20. Creatinine - (mg/dL)
21. Bilirubin_direct - Bilirubin direct (mg/dL)
22. Glucose - Serum glucose (mg/dL)
23. Lactate - Lactic acid (mg/dL)
24. Magnesium - (mmol/dL)
25. Phosphate - (mg/dL)
26. Potassium - (mmol/L)
27. Bilirubintotal - Total bilirubin (mg/dL)
28. TroponinI - Troponin I (ng/mL)
29. Hct - Hematocrit (%)
30. Hgb - Hemoglobin (g/dL)
31. PTT - partial thromboplastin time (seconds)
32. WBC - Leukocyte count (count10^3/µL)
33. Fibrinogen - (mg/dL)
34. Platelets - (count10^3/µL)
35. Age
36. Gender
37. Unit 1
38. Unit 2
39. HospAdmTime - Length of time they have been admitted
40. ICULOS - Length of time since admittance to ICU



"""

data = pd.read_csv('dataSepsis.csv', sep=';')

"""# EDA Continued
The following tables and plots display the majority of our data distributed log normal distributions, with a large number of our variables missing values. After reviewing commonly deployed techniques (Burdick et al.) the MICE (Multiple Imputation by Chained Equations) technique was commonly deployed to impute missing values. 
"""

data.describe()

data.hist(bins=100, figsize = (25,50))

data.corr(method='pearson')

correlation_table = data.corr().abs()

s = correlation_table.unstack()
so = s.sort_values(kind="quicksort")

print(so)

y = data.isna().sum()

total_count_of_values = pd.DataFrame(y)

total_count_sorted = total_count_of_values.sort_values(0)

total_count_sorted

total_count_sorted.plot.bar(ylabel ='Total count of missing values')

percent_missing_values = data.isna().mean().round(4) * 100

percent_missing_values.sort_values()

"""# Feature Engineering
My first approach to feature engineering came via a subsampling method of collecting rows with a certain percentage of missing values. Garnering a better understanding of the predictive capacity meant dropping a series of variables based on these metrics. Our threshold of ninety percent values availabe per column produced too few values (only one completed row) to be considered worthwhile and was dropped from the analysis. 

The second approach offered a significantly better system of determining variables worth keeping, and involved using the random number trick in conjuction with a random forest to see which values carried predictive information. If the values returned displayed a lower score than the row of random values, it can be assumed that they don't predict values more effectively than random chance. 
"""

Ninety_p_data = data.drop(['Fibrinogen', 'EtCO2', 'Bilirubin_direct'], axis=1)

Ninety_p_data_noNan = Ninety_p_data.dropna()

Ninety_p_data_noNan.describe()

Seventy_p_data = data.drop(['Bilirubin_direct', 'EtCO2', 'Fibrinogen', 'TroponinI', 'Lactate', 'SaO2', 'FiO2', 'BaseExcess', 'PaCO2', 'pH', 'Alkalinephos', 'Bilirubin_total', 'AST'], axis=1)

Seventy_p_data_noNan = Seventy_p_data.dropna()

Seventy_p_data_noNan.describe()

Fifty_p_data = data.drop(['Bilirubin_direct', 'EtCO2', 'Fibrinogen', 'TroponinI', 'Lactate', 'SaO2', 'FiO2', 'BaseExcess', 'PaCO2', 'pH', 'Alkalinephos', 'Bilirubin_total', 'AST', 'PTT', 'HCO3', 'Temp', 'Chloride'], axis=1)

Fifty_p_data_noNan = Fifty_p_data.dropna()

Fifty_p_data_noNan.describe()

#At this section I dropped the important values and then imputed them to test pre-post.

#Import imputed datasets and set up variable names for all the transformations (potential to explore more about distributions here)

"""# Exporting Data into R (MICE) and Row Dropping Transformations
While MICE can be conducted in python, R contains a simple package for deploying the MICE method and returning datasets of value. At this point in the analysis we had created views that contained only the columns above a certain percentage of values available. 

This system was enacted to develop six seperate views that progressively reduce the amount of bias introduced via the imputation methods. The result was the following datasets to be ingested into our keras binary classification. 



1.   Entire dataset with all values available imputed
2.   Columns containing at least 70% of values imputed

*   Columns dropped: 'Bilirubin_direct', 'EtCO2', 'Fibrinogen', 'TroponinI', 'Lactate', 'SaO2', 'FiO2', 'BaseExcess', 'PaCO2', 'pH', 'Alkalinephos', 'Bilirubin_total', 'AST'

3.   Columns containing at least 50% of values imputed

*   Columns dropped: 'Bilirubin_direct', 'EtCO2', 'Fibrinogen', 'TroponinI', 'Lactate', 'SaO2', 'FiO2', 'BaseExcess', 'PaCO2', 'pH', 'Alkalinephos', 'Bilirubin_total', 'AST', 'PTT', 'HCO3', 'Temp', 'Chloride'

4. All completed rows containing columns providing at least 70% of data points (756 completed rows)
5. All completed rows containing columns providing at least 50% of data points(7290 completed rows)
6. Feature extracted (random number trick) fully imputed dataset

The following code was deployed in R to conduct the imputation prior to being reimported. 

```
# library(mice)

md.pattern(dataSepsis)

library(VIM)

Ninety_p_values = subset(dataSepsis, select = -c('Fibrinogen', 'EtCO2', 'Bilirubin_direct'))


temp_fifty_imputed_data = mice(Fifty_p_data, m=5, maxit=50, meth='pmm', seed=500)

summary(temp_fifty_imputed_data)

completed_fifty_data = complete(temp_fifty_imputed_data, 1)

write.csv(completed_fifty_data, "datasepsis_fifty_impute")



full_data_imputed_temp = mice(dataSepsis, m=5, maxit=50, meth='pmm', seed=500)

completed_full_data = complete(full_data_imputed_temp, 1)

write.csv(completed_full_data, "datasepsis_complete_impute")



seventy_data_imputed_temp = mice(Seventy_p_data, m=5, maxit=50, meth='pmm', seed=500)

completed_seventy_data = complete(seventy_data_imputed_temp, 1)

write.csv(completed_seventy_data, "datasepsis_seventy_impute")

```
"""

datacomplete_impute = pd.read_csv('datasepsis_complete_impute', sep=',')

datacomplete_impute = datacomplete_impute.drop(['Unnamed: 0'], axis=1)

dataseventy_impute = pd.read_csv('datasepsis_seventy_impute', sep=',')

dataseventy_impute = dataseventy_impute.drop(['Unnamed: 0'], axis=1)

datafifty_impute = pd.read_csv('datasepsis_fifty_impute', sep=',')

datafifty_impute = datafifty_impute.drop(['Unnamed: 0'], axis=1)



#Split each subset into X and Y

SepsisCompleteImputed_y_val = datacomplete_impute['isSepsis']

SepsisCompleteImputed_x_vals = datacomplete_impute.drop(['isSepsis', 'Unit1', 'Unit2'], axis=1)



SepsisSeventyImputed_y_val = dataseventy_impute['isSepsis']

SepsisSeventyImputed_x_vals = dataseventy_impute.drop(['isSepsis', 'Unit1', 'Unit2'], axis=1)



SepsisFiftyImputed_y_val = datafifty_impute['isSepsis']

SepsisFiftyImputed_x_vals = datafifty_impute.drop(['isSepsis', 'Unit1', 'Unit2'], axis=1)



SepsisSeventy_y_val = Seventy_p_data_noNan['isSepsis']

SepsisSeventy_x_vals = Seventy_p_data_noNan.drop(['isSepsis', 'Unit1', 'Unit2'], axis=1)



SepsisFifty_y_val = Fifty_p_data_noNan['isSepsis']

SepsisFifty_x_vals = Fifty_p_data_noNan.drop(['isSepsis', 'Unit1', 'Unit2'], axis=1)



#Set up random number trick for feature selection

print(SepsisCompleteImputed_y_val.shape, SepsisCompleteImputed_x_vals.shape)



"""# Random Number Trick Deployment and Normalization 
The addition of a column of entirely random numbers was added to my dataframe prior to running it through two techniques for feature extraction. This was conducted prior to normalization to ensure no additional bias was added into my selection process. The datasets were then normalized to improve ML performance. 
"""

SepsisCompleteImputed_x_vals['randNumCol'] = np.random.randint(1, 1000, SepsisCompleteImputed_x_vals.shape[0])



#Scale all of the datasets between 0 and 1



x_fifty = SepsisFifty_x_vals.values
min_max_scalar = preprocessing.MinMaxScaler()
x_fifty_scaled = min_max_scalar.fit_transform(x_fifty)
data_fifty_complete_normalized = pd.DataFrame(x_fifty_scaled)



x_seventy = SepsisSeventy_x_vals.values
min_max_scalar = preprocessing.MinMaxScaler()
x_seventy_scaled = min_max_scalar.fit_transform(x_seventy)
data_seventy_complete_normalized = pd.DataFrame(x_seventy_scaled)



x_fifty_impute = SepsisFiftyImputed_x_vals.values
min_max_scalar = preprocessing.MinMaxScaler()
x_fifty_imputed_scaled = min_max_scalar.fit_transform(x_fifty_impute)
data_fifty_impute_normalized = pd.DataFrame(x_fifty_imputed_scaled)





x_seventy_impute = SepsisSeventyImputed_x_vals.values
min_max_scalar = preprocessing.MinMaxScaler()
x_seventy_imputed_scaled = min_max_scalar.fit_transform(x_seventy_impute)
data_seventy_impute_normalized = pd.DataFrame(x_seventy_imputed_scaled)



x_complete_impute = SepsisCompleteImputed_x_vals.values
min_max_scalar = preprocessing.MinMaxScaler()
x_complete_imputed_scaled = min_max_scalar.fit_transform(x_complete_impute)
datacomplete_impute_normalized = pd.DataFrame(x_complete_imputed_scaled)



#Random number trick for feature selection using the entire dataset

print(SepsisCompleteImputed_y_val.shape, SepsisCompleteImputed_x_vals.shape)



datacomplete_with_col_names = pd.DataFrame(data = datacomplete_impute_normalized.values, columns = SepsisCompleteImputed_x_vals.columns)



#split data into training and testing set for purpose of feature extraction

x_train,x_test,y_train,y_test = train_test_split(datacomplete_with_col_names, SepsisCompleteImputed_y_val, test_size=0.3)



print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)



nan_count =  x_train.isna().sum()

nan_count



"""#Random Number Trick Techniques


1.   Feature selection using SelectFromModel: This method is an sklearn provided method for organizing predictors based off importance. A mean value of importance is calculated and then used to determine if a variable is considered important or not. I elected to use the mean value as my cut off point, but further exploration could be done here to determine a better method (potential to be the outcome of the random number trick value) for selecting values of importance. 

    This produced the following values as important predictors: 'Temp', 'Calcium', 'Bilirubin_direct', 'Bilirubin_total', 'HospAdmTime',
       'ICULOS'
2.   Random forest run containing the random number column added earlier in the dataset. I ran this function five times and aggregated the values produced with respect to feature 38 (the column of random numbers). I elected to use these values produced because they serve as a better representation of predictive power than the mean importance value produced in SelectFromModel. The output produced the following values as important datapoints. I ran an additional run through the random forest using permutation importances, designed to hamper the negative effects of high cardinality. This solution provided less repeatable results and was not used in further exploration. 

   Finalized feature extracted predictors: 'HR', 'Temp', 'Calcium', 'MAP', 'PTT', 'HCO3', 'Creatinine', 'AST', 'Hgb', 'Hct', 'Fibrinogen', 'Platelets', 'BUN', 'Resp', 'WBC', 'Bilirubin_total', 'Bilirubin_direct', 'HospAdmTime', 'ICULOS'


"""

feature_selection_rf = SelectFromModel(RandomForestClassifier(n_estimators = 100))

feature_selection_rf.fit(x_train, y_train)

feature_selection_rf.get_support()

selected_feat= x_train.columns[(feature_selection_rf.get_support())]
len(selected_feat)

print(selected_feat)



feature_names = [f'feature{i}' for i in range(x_train.shape[1])]

forest = RandomForestClassifier(random_state=0)

forest.fit(x_train, y_train)

importances = forest.feature_importances_

std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)

forest_importances = pd.Series(importances, index=feature_names)

fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=std, ax=ax)
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()

forest_importances_sorted = forest_importances.sort_values()

forest_importances_sorted

forest_importances_sorted = pd.Series(importances, index=feature_names)

fig, ax = plt.subplots()
forest_importances_sorted.plot.bar(yerr=std, ax=ax)
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()

result = permutation_importance(forest, x_test, y_test, n_repeats=10, random_state=42, n_jobs=2)

forest_importances = pd.Series(result.importances_mean, index=feature_names)

fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()

forest_importances.sort_values()



model = RandomForestClassifier()

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

n_scores = cross_val_score(model, x_train, y_train, scoring='accuracy', cv=cv, n_jobs=1, error_score='raise')

print('Accuracy: %.3f (%.3f)' % (mean(n_scores), np.std(n_scores)))





datacomplete_with_col_names.head()

feature_extracted_predictors = datacomplete_with_col_names[['HR', 'Temp', 'Calcium', 'MAP', 'PTT', 'HCO3', 'Creatinine', 'AST', 'Hgb', 'Hct', 'Fibrinogen', 'Platelets', 'BUN', 'Resp', 'WBC', 'Bilirubin_total', 'Bilirubin_direct', 'HospAdmTime', 'ICULOS']]

feature_extracted_y_val = data['isSepsis']

feature_extracted_predictors.shape





"""# Initial Structuring for Keras Binary Classification 
To ensure we were effectively testing for the validity of our models I split the data into training and test sets and ran two different types of conclusionary validation. I elected to use both systems due to the variance in size that my different views provided. The datasets range from 36,000 datapoints all the way down to 750. The use of K-fold cross validation tends perform better on smaller datasets than the train-test split. Comparing both of these techniques allowed me to further investigate if the size of my dataset had an impact on model performance. 

1.   Stratified K-fold cross validation 
2.   Train-test split accuracy metrics after model fit


"""

#Creation of test/train splits with all of the current views

x_train_impute_complete,x_test_impute_complete,y_train_impute_complete,y_test_impute_complete = train_test_split(datacomplete_with_col_names, SepsisCompleteImputed_y_val, test_size=0.3)

x_train_impute_seventy,x_test_impute_seventy,y_train_impute_seventy,y_test_impute_seventy = train_test_split(data_seventy_impute_normalized, SepsisSeventyImputed_y_val, test_size=0.3)

data_seventy_impute_normalized.shape, SepsisSeventyImputed_y_val.shape

x_train_impute_fifty,x_test_impute_fifty,y_train_impute_fifty,y_test_impute_fifty = train_test_split(data_fifty_impute_normalized, SepsisFiftyImputed_y_val, test_size=0.3)

x_train_fifty,x_test_fifty,y_train_fifty,y_test_fifty = train_test_split(data_fifty_complete_normalized, SepsisFifty_y_val, test_size=0.3)

x_train_seventy,x_test_seventy,y_train_seventy,y_test_seventy = train_test_split(data_seventy_complete_normalized, SepsisSeventy_y_val, test_size=0.3)

x_train_seventy.shape, y_train_seventy.shape

x_train_feature_extracted,x_test_feature_extracted,y_train_feature_extracted,y_test_feature_extracted = train_test_split(feature_extracted_predictors, feature_extracted_y_val, test_size=0.3)







"""# Model Development using Keras
Due to the structuring of different views in my dataset I needed a series of sequential models that could handle the differing number of input parameters. This resulted in the construction of six relatively small models with input dimensions that reflect the correct number of variables. All other parameters of the models were held constant throughout testing. The models exist as follows:


1.   fully_imputed_model
2.   seventy_imputed_model
3.   fifty_imputed_model
4.   seventy_complete_model
5.   fifty_complete_model
6.   feature_extracted_model


"""

#Begin Machine Learning (Testing on fully imputed dataset prior to deploying model across views)

# baseline model
def create_baseline_full_dataset():
	# create model
	model = Sequential()
	model.add(Dense(60, input_dim=39, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model



# baseline model
def create_baseline_seventy_dataset():
	# create model
	model = Sequential()
	model.add(Dense(60, input_dim=26, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model



# baseline model
def create_baseline_fifty_dataset():
	# create model
	model = Sequential()
	model.add(Dense(60, input_dim=25, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model



# baseline model
def create_baseline_feature_extracted_dataset():
	# create model
	model = Sequential()
	model.add(Dense(60, input_dim=19, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model





# baseline model
def create_22_dataset():
	# create model
	model = Sequential()
	model.add(Dense(60, input_dim=22, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model



# baseline model
def create_21_dataset():
	# create model
	model = Sequential()
	model.add(Dense(60, input_dim=21, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model











fully_imputed_model = KerasClassifier(build_fn=create_baseline_full_dataset, epochs=10, batch_size=3, verbose=1)
kfold = StratifiedKFold(n_splits=10, shuffle=True)
results = cross_val_score(fully_imputed_model, x_test_impute_complete, y_test_impute_complete, cv=kfold)
print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

x_train_impute_seventy.shape, y_train_impute_seventy.shape

Seventy_imputed_model = KerasClassifier(build_fn=create_baseline_seventy_dataset, epochs=10, batch_size=3, verbose=1)
kfold = StratifiedKFold(n_splits=10, shuffle=True)
results = cross_val_score(Seventy_imputed_model, x_train_impute_seventy, y_train_impute_seventy, cv=kfold)
print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))



Seventy_complete_model = KerasClassifier(build_fn=create_baseline_fifty_dataset, epochs=10, batch_size=3, verbose=1)
kfold = StratifiedKFold(n_splits=10, shuffle=True)
results = cross_val_score(Seventy_complete_model, x_train_seventy, y_train_seventy, cv=kfold)
print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))



Fifty_imputed_model = KerasClassifier(build_fn=create_22_dataset, epochs=10, batch_size=3, verbose=1)
kfold = StratifiedKFold(n_splits=10, shuffle=True)
results = cross_val_score(Fifty_imputed_model, x_train_impute_fifty, y_train_impute_fifty, cv=kfold)
print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))



Fifty_complete_model = KerasClassifier(build_fn=create_21_dataset, epochs=10, batch_size=3, verbose=1)
kfold = StratifiedKFold(n_splits=10, shuffle=True)
results = cross_val_score(Fifty_complete_model, x_train_fifty, y_train_fifty, cv=kfold)
print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))



Feature_extracted_model = KerasClassifier(build_fn=create_baseline_feature_extracted_dataset, epochs=10, batch_size=3, verbose=1)
kfold = StratifiedKFold(n_splits=10, shuffle=True)
results = cross_val_score(Feature_extracted_model, x_train_feature_extracted, y_train_feature_extracted, cv=kfold)
print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

feature_extracted_predictors.shape, y_test_feature_extracted.shape, feature_extracted_y_val.shape

x_train_feature_extracted.shape, y_train_feature_extracted.shape







fully_imputed_model = KerasClassifier(build_fn=create_baseline_full_dataset, epochs=10, batch_size=3, verbose=1)

fully_imputed_model.fit(x_train_impute_complete, y_train_impute_complete)
yhat = fully_imputed_model.predict(x_test_impute_complete)
acc_full_impute = accuracy_score(y_test_impute_complete, yhat)
print('Accuracy: %.3f' % acc_full_impute)

acc_full_impute







Seventy_imputed_model.fit(x_train_impute_seventy, y_train_impute_seventy)
yhat = Seventy_imputed_model.predict(x_test_impute_seventy)
acc_seventy_impute = accuracy_score(y_test_impute_seventy, yhat)
print('Accuracy: %.3f' % acc_seventy_impute)





Seventy_complete_model.fit(x_train_seventy, y_train_seventy)
yhat = Seventy_complete_model.predict(x_test_seventy)
acc_seventy_impute = accuracy_score(y_test_seventy, yhat)
print('Accuracy: %.3f' % acc_seventy_impute)



Fifty_imputed_model.fit(x_train_impute_fifty, y_train_impute_fifty)
yhat = Fifty_imputed_model.predict(x_test_impute_fifty)
acc_fifty_impute = accuracy_score(y_test_impute_fifty, yhat)
print('Accuracy: %.3f' % acc_fifty_impute)



Fifty_complete_model.fit(x_train_fifty, y_train_fifty)
yhat = Fifty_complete_model.predict(x_test_fifty)
acc_fifty_complete = accuracy_score(y_test_fifty, yhat)
print('Accuracy: %.3f' % acc_fifty_complete)



#Feature extracted

Feature_extracted_model.fit(x_train_feature_extracted, y_train_feature_extracted)
yhat = Feature_extracted_model.predict(x_test_feature_extracted)
acc_fifty_complete = accuracy_score(y_test_feature_extracted, yhat)
print('Accuracy: %.3f' % acc_fifty_complete)



Crossfold_validation_outcomes = {'Fully Imputed Model': [95.23, 95.53], 'Seventy Percent Imputed Model': [95.29, 95.21], 'Seventy Percent Nan Dropped Model': [95.46, 95.25], 'Fifty Percent Imputed Model': [95.12, 95.14], 'Fifty Percent Nan Dropped Model': [97.81, 97.61], 'Feature Extracted Model': [95.12, 95.60]}

Train_test_split_accuracy_metric = {'Fully Imputed Model': [95.53], 'Seventy Percent Imputed Model': [95.21], 'Seventy Percent Nan Dropped Model': [95.25], 'Fifty Percent Imputed Model': [95.12], 'Fifty Percent Nan Dropped Model': [97.61], 'Feature Extracted Model': [95.60]}



Crossfold_validation_outcomes = pd.DataFrame(Crossfold_validation_outcomes)
Train_test_split_accuracy_metric = pd.DataFrame(Train_test_split_accuracy_metric)

Crossfold_validation_outcomes.index =['Cross-validation results', 'Train test split accuracy results']

"""# Conclusion
Imputation methods did very little to effect the performance of the model. The model that performed the best did so after the removal of roughly 29,000 datapoints (Fifty percent Nan dropped model). While the difference in performance was not significant, it could imply that imputing data to the extent we did in this project harmed the quality of the data.
"""

Crossfold_validation_outcomes

Final_outcomes = Crossfold_validation_outcomes.plot.bar(figsize = (13,13), rot=0)





"""# Part 2 - Sepsis prediction Model Performance on ICU and Hospital Admission Times

The purpose of this exploration was determining model performance across ICU and Hospital admission times. I elected to deploy a series of binary classification methods including naive bayes, logistic regression, an SVM and a keras deployed neural net. This determination of model performance could allow us to select the right type of model given the circumstance as well as build more effective tools for medical professionals to lean on when considering the risk of sepsis.
"""





from google.colab import files
files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels
from statsmodels.stats.weightstats import ztest as ztest
from numpy import mean
from numpy import std
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression


from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

data = pd.read_csv('dataSepsis.csv', sep=';')





"""# EDA Outcomes

My work in the previous practicum led me to the conclusion that imputation for training models actually decreased performance (by 3%). The result of this was training my current model with a selection of 7290 fully satiated datapoints, selected from variables that predicted outcomes better than a random number (random number trick). While the remaining 29000 data points were not used to train models, they were eventually imputed and used as an additional test dataset.
"""



data.hist(bins=100, figsize = (25,50))



df = pd.DataFrame(data, columns = ['ICULOS', 'HospAdmTime'])



df.hist(bins=100, figsize = (15,30))



"""# Binning System and Rationale

I elected to use 5 seperate bins to split my 7290 datapoints up, binning the values according to ICULOS and HospAdmTime. I averaged roughly 1400 datapoints per bin to ensure that my neural net had an equivolent chance to learn across bins. The following are functions I developed to bin the data.
"""





def bin_hosp_time(x):
    if x <= 2:
        return 1
    elif x > 2 and x <= 6:
        return 2
    elif x > 6 and x <= 20:
        return 3
    elif x > 20 and x <= 80:
        return 4
    else:
        return 5

def bin_iculos_time(x):
    if x <= 4:
        return 1
    elif x > 4 and x <= 8:
        return 2
    elif x > 8 and x <= 13:
        return 3
    elif x > 13 and x <= 21:
        return 4
    else:
        return 5











"""# Paritioning Data

**Fully satiated data (Fifty_p_data)**

*  These 7290 datapoints were cleaned and indexed prior to being binned. They are 
then passed through the binning system, re-indexed and then assigned to train test splits. The train test splits provided 70 percent of the data for training and kept the remaining 30 percent for testing. 

**Fully imputed data (Nan_data)**


*  The fully imputed data followed a similar protocol, without the train test split. The data was not trained with the fully imputed data, only tested against it.
"""



Fifty_p_data = data.drop(['Bilirubin_direct', 'EtCO2', 'Fibrinogen', 'TroponinI', 'Lactate', 'SaO2', 'FiO2', 'BaseExcess', 'PaCO2', 'pH', 'Alkalinephos', 'Bilirubin_total', 'AST', 'PTT', 'HCO3', 'Temp', 'Chloride'], axis=1)



Nan_data = Fifty_p_data[Fifty_p_data.isna().any(axis=1)]

Nan_data



Fifty_p_data_noNan = Fifty_p_data.dropna()

Fifty_p_data_noNan.describe()





Nan_data["HospAdmTimeAbs"] = Nan_data["HospAdmTime"].abs()



Nan_data





Nan_data["HospAdm_Bin"] = Nan_data["HospAdmTimeAbs"].apply(bin_hosp_time)

Nan_data["ICULOS_Bin"] = Nan_data["ICULOS"].apply(bin_iculos_time)

Nan_data_Iculos_bin = Nan_data['ICULOS_Bin']

Nan_data_Hosp_bin = Nan_data['HospAdm_Bin']

Nan_data_Iculos_bin = pd.DataFrame(Nan_data_Iculos_bin)

Nan_data_Hosp_bin = pd.DataFrame(Nan_data_Hosp_bin)

Nan_data_Hosp_bin









Fifty_p_data_noNan["HospAdmTimeAbs"] = Fifty_p_data_noNan["HospAdmTime"].abs()





Fifty_p_data_noNan["HospAdm_Bin"] = Fifty_p_data_noNan["HospAdmTimeAbs"].apply(bin_hosp_time)

Fifty_p_data_noNan["ICULOS_Bin"] = Fifty_p_data_noNan["ICULOS"].apply(bin_iculos_time)



Fifty_p_data_noNan.groupby('ICULOS_Bin').count()

Fifty_p_data_noNan.groupby('HospAdm_Bin').count()



Hosp_outcome = Fifty_p_data_noNan['HospAdm_Bin']

Iculos_outcome = Fifty_p_data_noNan['ICULOS_Bin']

Sepsis_outcome = Fifty_p_data_noNan['isSepsis']





Nan_data_y = Nan_data['isSepsis']

Nan_data_y = pd.DataFrame(Nan_data_y)

Nan_data_y

"""# MICE Imputation and Normalization

Both datasets were fed through a normalization process. The data that our model was not exposed to (Nan_data) was then exported and imputed with the MICE package in R. It was then reimported and relabeled to be ready for testing against models.
"""

Nan_data_normalized_x_original = pd.DataFrame(Nan_data.drop(['isSepsis', 'Unit1', 'Unit2'], axis=1))



Nan_data_normalized_x = Nan_data_normalized_x_original.values
min_max_scalar = preprocessing.MinMaxScaler()
Nan_normalization_data_complete = min_max_scalar.fit_transform(Nan_data_normalized_x)
Nan_data_normalized_x_complete = pd.DataFrame(Nan_normalization_data_complete)

Nan_data_with_col_names = pd.DataFrame(data = Nan_data_normalized_x_complete.values, columns = Nan_data_normalized_x_original.columns)

Nan_data_with_col_names = Nan_data_with_col_names.drop(['HospAdmTime', 'HospAdm_Bin', 'ICULOS_Bin'], axis=1)

Nan_data_with_col_names

adm_time = Nan_data_with_col_names['HospAdmTimeAbs']

adm_time = pd.DataFrame(adm_time)

from google.colab import files
Nan_data_with_col_names.to_csv('Nan_data_for_imputation.csv')
files.download("Nan_data_for_imputation.csv")





files.upload()

Nan_test_data = pd.read_csv('Nan_data_final.csv', sep=',')

Nan_test_data



Nan_test_data = Nan_test_data.drop(['Unnamed: 0', 'X'], axis=1)

Nan_test_data







Nan_test_data['HospAdmTimeAbs'] = adm_time

Nan_test_data



Nan_test_data = Nan_test_data.drop(['HospAdmTime'], axis=1)

Nan_test_data







normalization_data = Fifty_p_data_noNan.drop(['isSepsis', 'ICULOS_Bin', 'HospAdm_Bin'], axis=1)

normalization_data

normalized_data = normalization_data.values
min_max_scalar = preprocessing.MinMaxScaler()
normalization_data_complete = min_max_scalar.fit_transform(normalization_data)
data_fifty_complete_normalized = pd.DataFrame(normalization_data_complete)

data_fifty_complete_normalized

Fifty_p_data_noNan = pd.DataFrame(data = data_fifty_complete_normalized.values, columns = normalization_data.columns)

Fifty_p_data_noNan



Fifty_p_data_noNan



nan_data_index = range(0, 29012)

Nan_data_Iculos_bin['index'] = nan_data_index

Nan_data_Iculos_bin

Nan_data_Iculos_bin = Nan_data_Iculos_bin.set_index('index')

Nan_data_Hosp_bin['index'] = nan_data_index

Nan_data_Hosp_bin = Nan_data_Hosp_bin.set_index('index')

Nan_data_Hosp_bin

Nan_data_y['index'] = nan_data_index

Nan_data_y = Nan_data_y.set_index('index')

Nan_data_y

Nan_test_data['HospAdm_Bin'] = Nan_data_Hosp_bin

Nan_test_data['ICULOS_Bin'] = Nan_data_Iculos_bin

Nan_test_data['isSepsis'] = Nan_data_y

Nan_test_data









#Below is the code for adding indexes to Train test split data

new_index = range(0, 7290)

Hosp_outcome = pd.DataFrame(data = Hosp_outcome)

Hosp_outcome['index'] = new_index

Hosp_outcome = Hosp_outcome.set_index('index')

Iculos_outcome = pd.DataFrame(data= Iculos_outcome)

Iculos_outcome['index'] = new_index

Iculos_outcome = Iculos_outcome.set_index('index')



Sepsis_outcome = pd.DataFrame(data = Sepsis_outcome)

Sepsis_outcome['index'] = new_index

Sepsis_outcome





Sepsis_outcome = Sepsis_outcome.set_index('index')

Sepsis_outcome

Fifty_p_data_noNan['HospAdm_Bin'] = Hosp_outcome

Fifty_p_data_noNan['ICULOS_Bin'] = Iculos_outcome

Fifty_p_data_noNan['isSepsis'] = Sepsis_outcome

Fifty_p_data_noNan





def create_df_binning(df_value, column_name, bin_number):
    new_df = df_value.loc[df_value[column_name] == bin_number]
    return new_df

Fifty_p_data_noNan

I_bin_1 = create_df_binning(Fifty_p_data_noNan, column_name = 'ICULOS_Bin', bin_number = 1)
I_bin_2 = create_df_binning(Fifty_p_data_noNan, column_name = 'ICULOS_Bin', bin_number = 2)
I_bin_3 = create_df_binning(Fifty_p_data_noNan, column_name = 'ICULOS_Bin', bin_number = 3)
I_bin_4 = create_df_binning(Fifty_p_data_noNan, column_name = 'ICULOS_Bin', bin_number = 4)
I_bin_5 = create_df_binning(Fifty_p_data_noNan, column_name = 'ICULOS_Bin', bin_number = 5)

H_bin_1 = create_df_binning(Fifty_p_data_noNan, column_name = 'HospAdm_Bin', bin_number = 1)
H_bin_2 = create_df_binning(Fifty_p_data_noNan, column_name = 'HospAdm_Bin', bin_number = 2)
H_bin_3 = create_df_binning(Fifty_p_data_noNan, column_name = 'HospAdm_Bin', bin_number = 3)
H_bin_4 = create_df_binning(Fifty_p_data_noNan, column_name = 'HospAdm_Bin', bin_number = 4)
H_bin_5 = create_df_binning(Fifty_p_data_noNan, column_name = 'HospAdm_Bin', bin_number = 5)



#The following is the binning for the test dataset

I_bin_1_testset = create_df_binning(Nan_test_data, column_name = 'ICULOS_Bin', bin_number = 1)
I_bin_2_testset = create_df_binning(Nan_test_data, column_name = 'ICULOS_Bin', bin_number = 2)
I_bin_3_testset = create_df_binning(Nan_test_data, column_name = 'ICULOS_Bin', bin_number = 3)
I_bin_4_testset = create_df_binning(Nan_test_data, column_name = 'ICULOS_Bin', bin_number = 4)
I_bin_5_testset = create_df_binning(Nan_test_data, column_name = 'ICULOS_Bin', bin_number = 5)

H_bin_1_testset = create_df_binning(Nan_test_data, column_name = 'HospAdm_Bin', bin_number = 1)
H_bin_2_testset = create_df_binning(Nan_test_data, column_name = 'HospAdm_Bin', bin_number = 2)
H_bin_3_testset = create_df_binning(Nan_test_data, column_name = 'HospAdm_Bin', bin_number = 3)
H_bin_4_testset = create_df_binning(Nan_test_data, column_name = 'HospAdm_Bin', bin_number = 4)
H_bin_5_testset = create_df_binning(Nan_test_data, column_name = 'HospAdm_Bin', bin_number = 5)



I_bin_1_testset



def split_x_testset(bin_name):
    x_vals = bin_name.drop(['isSepsis', 'HospAdm_Bin', 'ICULOS_Bin'], axis=1)
    return x_vals





# def split_x(bin_name):
#     x_vals = bin_name.drop(['isSepsis'], axis=1)
#     return x_vals

def split_x(bin_name):
    x_vals = bin_name.drop(['isSepsis', 'Unit1', 'Unit2', 'HospAdmTime', 'HospAdm_Bin', 'ICULOS_Bin'], axis=1)
    return x_vals

def split_y(bin_name):
    y_vals = bin_name['isSepsis']
    return y_vals



I_bin_1_x = split_x(I_bin_1)
I_bin_1_y = split_y(I_bin_1)

I_bin_2_x = split_x(I_bin_2)
I_bin_2_y = split_y(I_bin_2)

I_bin_3_x = split_x(I_bin_3)
I_bin_3_y = split_y(I_bin_3)

I_bin_4_x = split_x(I_bin_4)
I_bin_4_y = split_y(I_bin_4)

I_bin_5_x = split_x(I_bin_5)
I_bin_5_y = split_y(I_bin_5)

H_bin_1_x = split_x(H_bin_1)
H_bin_1_y = split_y(H_bin_1)

H_bin_2_x = split_x(H_bin_2)
H_bin_2_y = split_y(H_bin_2)

H_bin_3_x = split_x(H_bin_3)
H_bin_3_y = split_y(H_bin_3)

H_bin_4_x = split_x(H_bin_4)
H_bin_4_y = split_y(H_bin_4)

H_bin_5_x = split_x(H_bin_5)
H_bin_5_y = split_y(H_bin_5)



#Below is the x/y split for the test set

I_bin_1_x_testset = split_x_testset(I_bin_1_testset)
I_bin_1_y_testset = split_y(I_bin_1_testset)

I_bin_2_x_testset = split_x_testset(I_bin_2_testset)
I_bin_2_y_testset = split_y(I_bin_2_testset)

I_bin_3_x_testset = split_x_testset(I_bin_3_testset)
I_bin_3_y_testset = split_y(I_bin_3_testset)

I_bin_4_x_testset = split_x_testset(I_bin_4_testset)
I_bin_4_y_testset = split_y(I_bin_4_testset)

I_bin_5_x_testset = split_x_testset(I_bin_5_testset)
I_bin_5_y_testset = split_y(I_bin_5_testset)

H_bin_1_x_testset = split_x_testset(H_bin_1_testset)
H_bin_1_y_testset = split_y(H_bin_1_testset)

H_bin_2_x_testset = split_x_testset(H_bin_2_testset)
H_bin_2_y_testset = split_y(H_bin_2_testset)

H_bin_3_x_testset = split_x_testset(H_bin_3_testset)
H_bin_3_y_testset = split_y(H_bin_3_testset)

H_bin_4_x_testset = split_x_testset(H_bin_4_testset)
H_bin_4_y_testset = split_y(H_bin_4_testset)

H_bin_5_x_testset = split_x_testset(H_bin_5_testset)
H_bin_5_y_testset = split_y(H_bin_5_testset)



X_train_I1, X_test_I1, Y_train_I1, Y_test_I1 = train_test_split(I_bin_1_x, I_bin_1_y, test_size=0.3)
X_train_I2, X_test_I2, Y_train_I2, Y_test_I2 = train_test_split(I_bin_2_x, I_bin_2_y, test_size=0.3)
X_train_I3, X_test_I3, Y_train_I3, Y_test_I3 = train_test_split(I_bin_3_x, I_bin_3_y, test_size=0.3)
X_train_I4, X_test_I4, Y_train_I4, Y_test_I4 = train_test_split(I_bin_4_x, I_bin_4_y, test_size=0.3)
X_train_I5, X_test_I5, Y_train_I5, Y_test_I5 = train_test_split(I_bin_5_x, I_bin_5_y, test_size=0.3)

X_train_H1, X_test_H1, Y_train_H1, Y_test_H1 = train_test_split(H_bin_1_x, H_bin_1_y, test_size=0.3)
X_train_H2, X_test_H2, Y_train_H2, Y_test_H2 = train_test_split(H_bin_2_x, H_bin_2_y, test_size=0.3)
X_train_H3, X_test_H3, Y_train_H3, Y_test_H3 = train_test_split(H_bin_3_x, H_bin_3_y, test_size=0.3)
X_train_H4, X_test_H4, Y_train_H4, Y_test_H4 = train_test_split(H_bin_4_x, H_bin_4_y, test_size=0.3)
X_train_H5, X_test_H5, Y_train_H5, Y_test_H5 = train_test_split(H_bin_5_x, H_bin_5_y, test_size=0.3)







"""# Model Deployments

The following models were selected for their prevalence in the literature and the variance they present in terms of compute required. 


1.   Naive Bayes 
2.   Logistic Regression 
3.   Support Vector Machine
4.   Keras Neural Net

These models were then tested against the testing set from my train-test split(Fifty_p_data), and then again against the data the model had not seen previously (Nan_data).
"""

from sklearn.naive_bayes import ComplementNB
from sklearn.naive_bayes import GaussianNB

clf = ComplementNB()
gnb = GaussianNB()





y_pred_I1_naivebayes = gnb.fit(X_train_I1, Y_train_I1).predict(X_test_I1)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I1.shape[0], (Y_test_I1 != y_pred_I1_naivebayes).sum()))

y_pred_correctly_labeled_I1_niavebayes = (X_test_I1.shape[0], (Y_test_I1 != y_pred_I1_naivebayes).sum()) 

y_pred_acc_I1_naivebayes = accuracy_score(Y_test_I1, y_pred_I1_naivebayes)
print(y_pred_acc_I1_naivebayes)

y_pred_I2_naivebayes = gnb.fit(X_train_I2, Y_train_I2).predict(X_test_I2)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I2.shape[0], (Y_test_I2 != y_pred_I2_naivebayes).sum()))

y_pred_correctly_labeled_I2_naivebayes = (X_test_I2.shape[0], (Y_test_I2 != y_pred_I2_naivebayes).sum()) 

y_pred_acc_I2_naivebayes = accuracy_score(Y_test_I2, y_pred_I2_naivebayes)
print(y_pred_acc_I2_naivebayes)

type(y_pred_I2_naivebayes)

print(X_train_I2.shape, I_bin_2_x_testset.shape)

X_train_I2

I_bin_2_x_testset





y_pred_I2_naivebayes_testset = gnb.fit(X_train_I2, Y_train_I2).predict(I_bin_2_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_I2_naivebayes_testset).sum()))

y_pred_correctly_labeled_I2_naivebayes_testset = (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_I2_naivebayes_testset).sum()) 

y_pred_acc_I2_naivebayes_testset = accuracy_score(I_bin_2_y_testset, y_pred_I2_naivebayes_testset)
print(y_pred_acc_I2_naivebayes_testset)

y_pred_I3_naivebayes = gnb.fit(X_train_I3, Y_train_I3).predict(X_test_I3)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I3.shape[0], (Y_test_I3 != y_pred_I3_naivebayes).sum()))

y_pred_correctly_labeled_I3_naivebayes = (X_test_I3.shape[0], (Y_test_I3 != y_pred_I3_naivebayes).sum()) 

y_pred_acc_I3_naivebayes = accuracy_score(Y_test_I3, y_pred_I3_naivebayes)
print(y_pred_acc_I3_naivebayes)

y_pred_I3_naivebayes_testset = gnb.fit(X_train_I3, Y_train_I3).predict(I_bin_3_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_I3_naivebayes_testset).sum()))

y_pred_correctly_labeled_I3_naivebayes_testset = (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_I3_naivebayes_testset).sum()) 

y_pred_acc_I3_naivebayes_testset = accuracy_score(I_bin_3_y_testset, y_pred_I3_naivebayes_testset)
print(y_pred_acc_I3_naivebayes_testset)





y_pred_I4_naivebayes = gnb.fit(X_train_I4, Y_train_I4).predict(X_test_I4)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I4.shape[0], (Y_test_I4 != y_pred_I4_naivebayes).sum()))

y_pred_correctly_labeled_I4_naivebayes = (X_test_I4.shape[0], (Y_test_I4 != y_pred_I4_naivebayes).sum()) 

y_pred_acc_I4_naivebayes = accuracy_score(Y_test_I4, y_pred_I4_naivebayes)
print(y_pred_acc_I4_naivebayes)

y_pred_I4_naivebayes_testset = gnb.fit(X_train_I4, Y_train_I4).predict(I_bin_4_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_I4_naivebayes_testset).sum()))

y_pred_correctly_labeled_I4_naivebayes_testset = (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_I4_naivebayes_testset).sum()) 

y_pred_acc_I4_naivebayes_testset = accuracy_score(I_bin_4_y_testset, y_pred_I4_naivebayes_testset)
print(y_pred_acc_I4_naivebayes_testset)

y_pred_I5_naivebayes = gnb.fit(X_train_I5, Y_train_I5).predict(X_test_I5)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I5.shape[0], (Y_test_I5 != y_pred_I5_naivebayes).sum()))

y_pred_correctly_labeled_I5_naivebayes = (X_test_I5.shape[0], (Y_test_I5 != y_pred_I5_naivebayes).sum()) 

y_pred_acc_I5_naivebayes = accuracy_score(Y_test_I5, y_pred_I5_naivebayes)
print(y_pred_acc_I5_naivebayes)

y_pred_I5_naivebayes_testset = gnb.fit(X_train_I5, Y_train_I5).predict(I_bin_5_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_I5_naivebayes_testset).sum()))

y_pred_correctly_labeled_I5_naivebayes_testset = (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_I5_naivebayes_testset).sum()) 

y_pred_acc_I5_naivebayes_testset = accuracy_score(I_bin_5_y_testset, y_pred_I5_naivebayes_testset)
print(y_pred_acc_I5_naivebayes_testset)

"""# New Section"""

y_pred_H1_naivebayes = gnb.fit(X_train_H1, Y_train_H1).predict(X_test_H1)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H1.shape[0], (Y_test_H1 != y_pred_H1_naivebayes).sum()))

y_pred_correctly_labeled_H1_naivebayes = (X_test_H1.shape[0], (Y_test_H1 != y_pred_H1_naivebayes).sum()) 

y_pred_acc_H1_naivebayes = accuracy_score(Y_test_H1, y_pred_H1_naivebayes)
print(y_pred_acc_H1_naivebayes)

y_pred_H1_naivebayes_testset = gnb.fit(X_train_H1, Y_train_H1).predict(I_bin_1_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_1_x_testset.shape[0], (I_bin_1_y_testset != y_pred_H1_naivebayes_testset).sum()))

y_pred_correctly_labeled_H1_naivebayes_testset = (I_bin_1_x_testset.shape[0], (I_bin_1_y_testset != y_pred_H1_naivebayes_testset).sum()) 

y_pred_acc_H1_naivebayes_testset = accuracy_score(I_bin_1_y_testset, y_pred_H1_naivebayes_testset)
print(y_pred_acc_H1_naivebayes_testset)

y_pred_H2_naivebayes = gnb.fit(X_train_H2, Y_train_H2).predict(X_test_H2)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H2.shape[0], (Y_test_H2 != y_pred_H2_naivebayes).sum()))

y_pred_correctly_labeled_H2_naivebayes = (X_test_H2.shape[0], (Y_test_H2 != y_pred_H2_naivebayes).sum()) 

y_pred_acc_H2_naivebayes = accuracy_score(Y_test_H2, y_pred_H2_naivebayes)
print(y_pred_acc_H2_naivebayes)

y_pred_H2_naivebayes_testset = gnb.fit(X_train_H2, Y_train_H2).predict(I_bin_2_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_H2_naivebayes_testset).sum()))

y_pred_correctly_labeled_H2_naivebayes_testset = (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_H2_naivebayes_testset).sum()) 

y_pred_acc_H2_naivebayes_testset = accuracy_score(I_bin_2_y_testset, y_pred_H2_naivebayes_testset)
print(y_pred_acc_H2_naivebayes_testset)

y_pred_H3_naivebayes = gnb.fit(X_train_H3, Y_train_H3).predict(X_test_H3)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H3.shape[0], (Y_test_H3 != y_pred_H3_naivebayes).sum()))

y_pred_correctly_labeled_H3_naivebayes = (X_test_H3.shape[0], (Y_test_H3 != y_pred_H3_naivebayes).sum()) 

y_pred_acc_H3_naivebayes = accuracy_score(Y_test_H3, y_pred_H3_naivebayes)
print(y_pred_acc_H3_naivebayes)

y_pred_H3_naivebayes_testset = gnb.fit(X_train_H3, Y_train_H3).predict(I_bin_3_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_H3_naivebayes_testset).sum()))

y_pred_correctly_labeled_H3_naivebayes_testset = (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_H3_naivebayes_testset).sum()) 

y_pred_acc_H3_naivebayes_testset = accuracy_score(I_bin_3_y_testset, y_pred_H3_naivebayes_testset)
print(y_pred_acc_H3_naivebayes_testset)

y_pred_H4_naivebayes = gnb.fit(X_train_H4, Y_train_H4).predict(X_test_H4)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H4.shape[0], (Y_test_H4 != y_pred_H4_naivebayes).sum()))

y_pred_correctly_labeled_H4_naivebayes = (X_test_H4.shape[0], (Y_test_H4 != y_pred_H4_naivebayes).sum()) 

y_pred_acc_H4_naivebayes = accuracy_score(Y_test_H4, y_pred_H4_naivebayes)
print(y_pred_acc_H4_naivebayes)

y_pred_H4_naivebayes_testset = gnb.fit(X_train_H4, Y_train_H4).predict(I_bin_4_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_H4_naivebayes_testset).sum()))

y_pred_correctly_labeled_H4_naivebayes_testset = (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_H4_naivebayes_testset).sum()) 

y_pred_acc_H4_naivebayes_testset = accuracy_score(I_bin_4_y_testset, y_pred_H4_naivebayes_testset)
print(y_pred_acc_H4_naivebayes_testset)

y_pred_H5_naivebayes = gnb.fit(X_train_H5, Y_train_H5).predict(X_test_H5)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H5.shape[0], (Y_test_H5 != y_pred_H5_naivebayes).sum()))

y_pred_correctly_labeled_H5_naivebayes = (X_test_H5.shape[0], (Y_test_H5 != y_pred_H5_naivebayes).sum()) 

y_pred_acc_H5_naivebayes = accuracy_score(Y_test_H5, y_pred_H5_naivebayes)
print(y_pred_acc_H5_naivebayes)

y_pred_H5_naivebayes_testset = gnb.fit(X_train_H5, Y_train_H5).predict(I_bin_5_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_H5_naivebayes_testset).sum()))

y_pred_correctly_labeled_H5_naivebayes_testset = (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_H5_naivebayes_testset).sum()) 

y_pred_acc_H5_naivebayes_testset = accuracy_score(I_bin_5_y_testset, y_pred_H5_naivebayes_testset)
print(y_pred_acc_H5_naivebayes_testset)





Naive_bayes_Iculos = [y_pred_acc_I1_naivebayes, y_pred_acc_I2_naivebayes, y_pred_acc_I3_naivebayes, y_pred_acc_I4_naivebayes, y_pred_acc_I5_naivebayes]

Naive_bayes_HospAdmTime = [y_pred_acc_H1_naivebayes, y_pred_acc_H2_naivebayes, y_pred_acc_H3_naivebayes, y_pred_acc_H4_naivebayes, y_pred_acc_H5_naivebayes]

# Naive_bayes_hospadmtime.plot()

Naive_bayes_Iculos_testset = [1, y_pred_acc_I2_naivebayes_testset, y_pred_acc_I3_naivebayes_testset, y_pred_acc_I4_naivebayes_testset, y_pred_acc_I5_naivebayes_testset]

Naive_bayes_HospAdm_testset = [y_pred_acc_H1_naivebayes_testset, y_pred_acc_H2_naivebayes_testset, y_pred_acc_H3_naivebayes_testset, y_pred_acc_H4_naivebayes_testset, y_pred_acc_H5_naivebayes_testset]





#Logistic regression

from sklearn import metrics

logreg = LogisticRegression()

"""#ypred1 only has zeros, resulting in this error below. """

# y_pred_I1_logreg = logreg.fit(X_train_I1, Y_train_I1).predict(X_test_I1)

# print("Number of mislabeled points out of a total %d points : %d" % (X_test_I1.shape[0], (Y_test_I1 != y_pred_I1_logreg).sum()))

# y_pred_correctly_labeled_I1_logreg = (X_test_I1.shape[0], (Y_test_I1 != y_pred_I1_logreg).sum()) 

# y_pred_acc_I1_logreg = accuracy_score(Y_test_I1, y_pred_I1_logreg)
# print(y_pred_acc_I1_logreg)

y_pred_I2_logreg = logreg.fit(X_train_I2, Y_train_I2).predict(X_test_I2)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I2.shape[0], (Y_test_I2 != y_pred_I2_logreg).sum()))

y_pred_correctly_labeled_I2_logreg = (X_test_I2.shape[0], (Y_test_I2 != y_pred_I2_logreg).sum()) 

y_pred_acc_I2_logreg = accuracy_score(Y_test_I2, y_pred_I2_logreg)
print(y_pred_acc_I2_logreg)

y_pred_I2_logreg_testset = logreg.fit(X_train_I2, Y_train_I2).predict(I_bin_2_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_I2_logreg_testset).sum()))

y_pred_correctly_labeled_I2_logreg_testset = (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_I2_logreg_testset).sum()) 

y_pred_acc_I2_logreg_testset = accuracy_score(I_bin_2_y_testset, y_pred_I2_logreg_testset)
print(y_pred_acc_I2_logreg_testset)

y_pred_I3_logreg = logreg.fit(X_train_I3, Y_train_I3).predict(X_test_I3)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I3.shape[0], (Y_test_I3 != y_pred_I3_logreg).sum()))

y_pred_correctly_labeled_I3_logreg = (X_test_I3.shape[0], (Y_test_I3 != y_pred_I3_logreg).sum()) 

y_pred_acc_I3_logreg = accuracy_score(Y_test_I3, y_pred_I3_logreg)
print(y_pred_acc_I3_logreg)

y_pred_I3_logreg_testset = logreg.fit(X_train_I3, Y_train_I3).predict(I_bin_3_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_I3_logreg_testset).sum()))

y_pred_correctly_labeled_I3_logreg_testset = (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_I3_logreg_testset).sum()) 

y_pred_acc_I3_logreg_testset = accuracy_score(I_bin_3_y_testset, y_pred_I3_logreg_testset)
print(y_pred_acc_I3_logreg_testset)





y_pred_I4_logreg = logreg.fit(X_train_I4, Y_train_I4).predict(X_test_I4)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I4.shape[0], (Y_test_I4 != y_pred_I4_logreg).sum()))

y_pred_correctly_labeled_I4_logreg = (X_test_I4.shape[0], (Y_test_I4 != y_pred_I4_logreg).sum()) 

y_pred_acc_I4_logreg = accuracy_score(Y_test_I4, y_pred_I4_logreg)
print(y_pred_acc_I4_logreg)

y_pred_I4_logreg_testset = logreg.fit(X_train_I4, Y_train_I4).predict(I_bin_4_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_I4_logreg_testset).sum()))

y_pred_correctly_labeled_I4_logreg_testset = (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_I4_logreg_testset).sum()) 

y_pred_acc_I4_logreg_testset = accuracy_score(I_bin_4_y_testset, y_pred_I4_logreg_testset)
print(y_pred_acc_I4_logreg_testset)

y_pred_I5_logreg = logreg.fit(X_train_I5, Y_train_I5).predict(X_test_I5)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I5.shape[0], (Y_test_I5 != y_pred_I5_logreg).sum()))

y_pred_correctly_labeled_I5_logreg = (X_test_I5.shape[0], (Y_test_I5 != y_pred_I5_logreg).sum()) 

y_pred_acc_I5_logreg = accuracy_score(Y_test_I5, y_pred_I5_logreg)
print(y_pred_acc_I5_logreg)

y_pred_I5_logreg_testset = logreg.fit(X_train_I5, Y_train_I5).predict(I_bin_5_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_I5_logreg_testset).sum()))

y_pred_correctly_labeled_I5_logreg_testset = (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_I5_logreg_testset).sum()) 

y_pred_acc_I5_logreg_testset = accuracy_score(I_bin_5_y_testset, y_pred_I5_logreg_testset)
print(y_pred_acc_I5_logreg_testset)



y_pred_H1_logreg = logreg.fit(X_train_H1, Y_train_H1).predict(X_test_H1)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H1.shape[0], (Y_test_H1 != y_pred_H1_logreg).sum()))

y_pred_correctly_labeled_H1_logreg = (X_test_H1.shape[0], (Y_test_H1 != y_pred_H1_logreg).sum()) 

y_pred_acc_H1_logreg = accuracy_score(Y_test_H1, y_pred_H1_logreg)
print(y_pred_acc_H1_logreg)

y_pred_H1_logreg_testset = logreg.fit(X_train_H1, Y_train_H1).predict(I_bin_1_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_1_x_testset.shape[0], (I_bin_1_y_testset != y_pred_H1_logreg_testset).sum()))

y_pred_correctly_labeled_H1_logreg_testset = (I_bin_1_x_testset.shape[0], (I_bin_1_y_testset != y_pred_H1_logreg_testset).sum()) 

y_pred_acc_H1_logreg_testset = accuracy_score(I_bin_1_y_testset, y_pred_H1_logreg_testset)
print(y_pred_acc_H1_logreg_testset)

y_pred_H2_logreg = logreg.fit(X_train_H2, Y_train_H2).predict(X_test_H2)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H2.shape[0], (Y_test_H2 != y_pred_H2_logreg).sum()))

y_pred_correctly_labeled_H2_logreg = (X_test_H2.shape[0], (Y_test_H2 != y_pred_H2_logreg).sum()) 

y_pred_acc_H2_logreg = accuracy_score(Y_test_H2, y_pred_H2_logreg)
print(y_pred_acc_H2_logreg)

y_pred_H2_logreg_testset = logreg.fit(X_train_H2, Y_train_H2).predict(I_bin_2_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_H2_logreg_testset).sum()))

y_pred_correctly_labeled_H2_logreg_testset = (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_H2_logreg_testset).sum()) 

y_pred_acc_H2_logreg_testset = accuracy_score(I_bin_2_y_testset, y_pred_H2_logreg_testset)
print(y_pred_acc_H2_logreg_testset)

y_pred_H3_logreg = logreg.fit(X_train_H3, Y_train_H3).predict(X_test_H3)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H3.shape[0], (Y_test_H3 != y_pred_H3_logreg).sum()))

y_pred_correctly_labeled_H3_logreg = (X_test_H3.shape[0], (Y_test_H3 != y_pred_H3_logreg).sum()) 

y_pred_acc_H3_logreg = accuracy_score(Y_test_H3, y_pred_H3_logreg)
print(y_pred_acc_H3_logreg)

y_pred_H3_logreg_testset = logreg.fit(X_train_H3, Y_train_H3).predict(I_bin_3_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_H3_logreg_testset).sum()))

y_pred_correctly_labeled_H3_logreg_testset = (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_H3_logreg_testset).sum()) 

y_pred_acc_H3_logreg_testset = accuracy_score(I_bin_3_y_testset, y_pred_H3_logreg_testset)
print(y_pred_acc_H3_logreg_testset)

y_pred_H4_logreg = logreg.fit(X_train_H4, Y_train_H4).predict(X_test_H4)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H4.shape[0], (Y_test_H4 != y_pred_H4_logreg).sum()))

y_pred_correctly_labeled_H4_logreg = (X_test_H4.shape[0], (Y_test_H4 != y_pred_H4_logreg).sum()) 

y_pred_acc_H4_logreg = accuracy_score(Y_test_H4, y_pred_H4_logreg)
print(y_pred_acc_H4_logreg)

y_pred_H4_logreg_testset = logreg.fit(X_train_H4, Y_train_H4).predict(I_bin_4_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_H4_logreg_testset).sum()))

y_pred_correctly_labeled_H4_logreg_testset = (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_H4_logreg_testset).sum()) 

y_pred_acc_H4_logreg_testset = accuracy_score(I_bin_4_y_testset, y_pred_H4_logreg_testset)
print(y_pred_acc_H4_logreg_testset)

y_pred_H5_logreg = logreg.fit(X_train_H5, Y_train_H5).predict(X_test_H5)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H5.shape[0], (Y_test_H5 != y_pred_H5_logreg).sum()))

y_pred_correctly_labeled_H5_logreg = (X_test_H5.shape[0], (Y_test_H5 != y_pred_H5_logreg).sum()) 

y_pred_acc_H5_logreg = accuracy_score(Y_test_H5, y_pred_H5_logreg)
print(y_pred_acc_H5_logreg)

y_pred_H5_logreg_testset = logreg.fit(X_train_H5, Y_train_H5).predict(I_bin_5_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_H5_logreg_testset).sum()))

y_pred_correctly_labeled_H5_logreg_testset = (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_H5_logreg_testset).sum()) 

y_pred_acc_H5_logreg_testset = accuracy_score(I_bin_5_y_testset, y_pred_H5_logreg_testset)
print(y_pred_acc_H5_logreg_testset)



Logreg_Iculos = [1 , y_pred_acc_I2_logreg, y_pred_acc_I3_logreg, y_pred_acc_I4_logreg, y_pred_acc_I5_logreg]

Logreg_HospAdmTime = [y_pred_acc_H1_logreg, y_pred_acc_H2_logreg, y_pred_acc_H3_logreg, y_pred_acc_H4_logreg, y_pred_acc_H5_logreg]

Logreg_Iculos_testset = [1, y_pred_acc_I2_logreg_testset, y_pred_acc_I3_logreg_testset, y_pred_acc_I4_logreg_testset, y_pred_acc_I5_logreg_testset]

Logreg_HospAdmTime_testset = [y_pred_acc_H1_logreg_testset, y_pred_acc_H2_logreg_testset, y_pred_acc_H3_logreg_testset, y_pred_acc_H4_logreg_testset, y_pred_acc_H5_logreg_testset]





#K-Nearest neighbors - Bad idea for highly dimensional data









#SVM

from sklearn import svm

clf = svm.SVC()

y_pred_I2_clf = clf.fit(X_train_I2, Y_train_I2).predict(X_test_I2)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I2.shape[0], (Y_test_I2 != y_pred_I2_clf).sum()))

y_pred_correctly_labeled_I2_clf = (X_test_I2.shape[0], (Y_test_I2 != y_pred_I2_clf).sum()) 

y_pred_acc_I2_clf = accuracy_score(Y_test_I2, y_pred_I2_clf)
print(y_pred_acc_I2_clf)

y_pred_I2_clf_testset = clf.fit(X_train_I2, Y_train_I2).predict(I_bin_2_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_I2_clf_testset).sum()))

y_pred_correctly_labeled_I2_clf_testset = (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_I2_clf_testset).sum()) 

y_pred_acc_I2_clf_testset = accuracy_score(I_bin_2_y_testset, y_pred_I2_clf_testset)
print(y_pred_acc_I2_clf_testset)

y_pred_I3_clf = clf.fit(X_train_I3, Y_train_I3).predict(X_test_I3)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I3.shape[0], (Y_test_I3 != y_pred_I3_clf).sum()))

y_pred_correctly_labeled_I3_clf = (X_test_I3.shape[0], (Y_test_I3 != y_pred_I3_clf).sum()) 

y_pred_acc_I3_clf = accuracy_score(Y_test_I3, y_pred_I3_clf)
print(y_pred_acc_I3_clf)

y_pred_I3_clf_testset = clf.fit(X_train_I3, Y_train_I3).predict(I_bin_3_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_I3_clf_testset).sum()))

y_pred_correctly_labeled_I3_clf_testset = (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_I3_clf_testset).sum()) 

y_pred_acc_I3_clf_testset = accuracy_score(I_bin_3_y_testset, y_pred_I3_clf_testset)
print(y_pred_acc_I3_clf_testset)

y_pred_I4_clf = clf.fit(X_train_I4, Y_train_I4).predict(X_test_I4)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I4.shape[0], (Y_test_I4 != y_pred_I4_clf).sum()))

y_pred_correctly_labeled_I4_clf = (X_test_I4.shape[0], (Y_test_I4 != y_pred_I4_clf).sum()) 

y_pred_acc_I4_clf = accuracy_score(Y_test_I4, y_pred_I4_clf)
print(y_pred_acc_I4_clf)

y_pred_I4_clf_testset = clf.fit(X_train_I4, Y_train_I4).predict(I_bin_4_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_I4_clf_testset).sum()))

y_pred_correctly_labeled_I4_clf_testset = (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_I4_clf_testset).sum()) 

y_pred_acc_I4_clf_testset = accuracy_score(I_bin_4_y_testset, y_pred_I4_clf_testset)
print(y_pred_acc_I4_clf_testset)

y_pred_I5_clf = clf.fit(X_train_I5, Y_train_I5).predict(X_test_I5)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_I5.shape[0], (Y_test_I5 != y_pred_I5_clf).sum()))

y_pred_correctly_labeled_I5_clf = (X_test_I5.shape[0], (Y_test_I5 != y_pred_I5_clf).sum()) 

y_pred_acc_I5_clf = accuracy_score(Y_test_I5, y_pred_I5_clf)
print(y_pred_acc_I5_clf)

y_pred_I5_clf_testset = clf.fit(X_train_I5, Y_train_I5).predict(I_bin_5_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_I5_clf_testset).sum()))

y_pred_correctly_labeled_I5_clf_testset = (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_I5_clf_testset).sum()) 

y_pred_acc_I5_clf_testset = accuracy_score(I_bin_5_y_testset, y_pred_I5_clf_testset)
print(y_pred_acc_I5_clf_testset)



y_pred_H1_clf = clf.fit(X_train_H1, Y_train_H1).predict(X_test_H1)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H1.shape[0], (Y_test_H1 != y_pred_H1_clf).sum()))

y_pred_correctly_labeled_H1_clf = (X_test_H1.shape[0], (Y_test_H1 != y_pred_H1_clf).sum()) 

y_pred_acc_H1_clf = accuracy_score(Y_test_H1, y_pred_H1_clf)
print(y_pred_acc_H1_clf)

y_pred_H1_clf_testset = clf.fit(X_train_H1, Y_train_H1).predict(I_bin_1_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_1_x_testset.shape[0], (I_bin_1_y_testset != y_pred_H1_clf_testset).sum()))

y_pred_correctly_labeled_H1_clf_testset = (I_bin_1_x_testset.shape[0], (I_bin_1_y_testset != y_pred_H1_clf_testset).sum()) 

y_pred_acc_H1_clf_testset = accuracy_score(I_bin_1_y_testset, y_pred_H1_clf_testset)
print(y_pred_acc_H1_clf_testset)

y_pred_H2_clf = clf.fit(X_train_H2, Y_train_H2).predict(X_test_H2)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H2.shape[0], (Y_test_H2 != y_pred_H2_clf).sum()))

y_pred_correctly_labeled_H2_clf = (X_test_H2.shape[0], (Y_test_H2 != y_pred_H2_clf).sum()) 

y_pred_acc_H2_clf = accuracy_score(Y_test_H2, y_pred_H2_clf)
print(y_pred_acc_H2_clf)

y_pred_H2_clf_testset = clf.fit(X_train_H2, Y_train_H2).predict(I_bin_2_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_H2_clf_testset).sum()))

y_pred_correctly_labeled_H2_clf_testset = (I_bin_2_x_testset.shape[0], (I_bin_2_y_testset != y_pred_H2_clf_testset).sum()) 

y_pred_acc_H2_clf_testset = accuracy_score(I_bin_2_y_testset, y_pred_H2_clf_testset)
print(y_pred_acc_H2_clf_testset)

y_pred_H3_clf = clf.fit(X_train_H3, Y_train_H3).predict(X_test_H3)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H3.shape[0], (Y_test_H3 != y_pred_H3_clf).sum()))

y_pred_correctly_labeled_H3_clf = (X_test_H3.shape[0], (Y_test_H3 != y_pred_H3_clf).sum()) 

y_pred_acc_H3_clf = accuracy_score(Y_test_H3, y_pred_H3_clf)
print(y_pred_acc_H3_clf)

y_pred_H3_clf_testset = clf.fit(X_train_H3, Y_train_H3).predict(I_bin_3_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_H3_clf_testset).sum()))

y_pred_correctly_labeled_H3_clf_testset = (I_bin_3_x_testset.shape[0], (I_bin_3_y_testset != y_pred_H3_clf_testset).sum()) 

y_pred_acc_H3_clf_testset = accuracy_score(I_bin_3_y_testset, y_pred_H3_clf_testset)
print(y_pred_acc_H3_clf_testset)

y_pred_H4_clf = clf.fit(X_train_H4, Y_train_H4).predict(X_test_H4)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H4.shape[0], (Y_test_H4 != y_pred_H4_clf).sum()))

y_pred_correctly_labeled_H4_clf = (X_test_H4.shape[0], (Y_test_H4 != y_pred_H4_clf).sum()) 

y_pred_acc_H4_clf = accuracy_score(Y_test_H4, y_pred_H4_clf)
print(y_pred_acc_H4_clf)

y_pred_H4_clf_testset = clf.fit(X_train_H4, Y_train_H4).predict(I_bin_4_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_H4_clf_testset).sum()))

y_pred_correctly_labeled_H4_clf_testset = (I_bin_4_x_testset.shape[0], (I_bin_4_y_testset != y_pred_H4_clf_testset).sum()) 

y_pred_acc_H4_clf_testset = accuracy_score(I_bin_4_y_testset, y_pred_H4_clf_testset)
print(y_pred_acc_H4_clf_testset)

y_pred_H5_clf = clf.fit(X_train_H5, Y_train_H5).predict(X_test_H5)

print("Number of mislabeled points out of a total %d points : %d" % (X_test_H5.shape[0], (Y_test_H5 != y_pred_H5_clf).sum()))

y_pred_correctly_labeled_H5_clf = (X_test_H5.shape[0], (Y_test_H5 != y_pred_H5_clf).sum()) 

y_pred_acc_H5_clf = accuracy_score(Y_test_H5, y_pred_H5_clf)
print(y_pred_acc_H5_clf)

y_pred_H5_clf_testset = clf.fit(X_train_H5, Y_train_H5).predict(I_bin_5_x_testset)

print("Number of mislabeled points out of a total %d points : %d" % (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_H5_clf_testset).sum()))

y_pred_correctly_labeled_H5_clf_testset = (I_bin_5_x_testset.shape[0], (I_bin_5_y_testset != y_pred_H5_clf_testset).sum()) 

y_pred_acc_H5_clf_testset = accuracy_score(I_bin_5_y_testset, y_pred_H5_clf_testset)
print(y_pred_acc_H5_clf_testset)



SVM_Iculos = [1 , y_pred_acc_I2_clf, y_pred_acc_I3_clf, y_pred_acc_I4_clf, y_pred_acc_I5_clf]

SVM_HospAdmTime = [y_pred_acc_H1_clf , y_pred_acc_H2_clf, y_pred_acc_H3_clf, y_pred_acc_H4_clf, y_pred_acc_H5_clf]

SVM_Iculos_testset = [1, y_pred_acc_I2_clf_testset, y_pred_acc_I3_clf_testset, y_pred_acc_I4_clf_testset, y_pred_acc_I5_clf_testset]

SVM_HospAdm_testset = [y_pred_acc_H1_clf_testset, y_pred_acc_H2_clf_testset, y_pred_acc_H3_clf_testset, y_pred_acc_H4_clf_testset, y_pred_acc_H5_clf_testset]



X_train_H1

#CNN





model = Sequential()
model.add(Dense(60, input_dim=21, activation='relu'))
model.add(Dense(30, activation = 'relu' ))
model.add(Dense(1, activation='sigmoid'))
	# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_H1, Y_train_H1, epochs=20)

pred_train= model.predict(X_train_H1)
scores = model.evaluate(X_train_H1, Y_train_H1, verbose=0)
print('Accuracy on training data: {}% \n Error on training data: {}'.format(scores[1], 1 - scores[1]))   
 
pred_test= model.predict(X_test_H1)
y_pred_acc_H1_cnn = model.evaluate(X_test_H1, Y_test_H1, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_H1_cnn[1], 1 - y_pred_acc_H1_cnn[1]))       


pred_test= model.predict(H_bin_1_x_testset)
y_pred_acc_H1_cnn_testset = model.evaluate(H_bin_1_x_testset, H_bin_1_y_testset, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_H1_cnn_testset[1], 1 - y_pred_acc_H1_cnn_testset[1]))

model = Sequential()
model.add(Dense(60, input_dim=21, activation='relu'))
model.add(Dense(30, activation = 'relu' ))
model.add(Dense(1, activation='sigmoid'))
	# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_H2, Y_train_H2, epochs=20)

pred_train= model.predict(X_train_H2)
scores = model.evaluate(X_train_H2, Y_train_H2, verbose=0)
print('Accuracy on training data: {}% \n Error on training data: {}'.format(scores[1], 1 - scores[1]))   
 
pred_test= model.predict(X_test_H2)
y_pred_acc_H2_cnn = model.evaluate(X_test_H2, Y_test_H2, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_H2_cnn[1], 1 - y_pred_acc_H2_cnn[1]))  

pred_test= model.predict(H_bin_2_x_testset)
y_pred_acc_H2_cnn_testset = model.evaluate(H_bin_2_x_testset, H_bin_2_y_testset, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_H2_cnn_testset[1], 1 - y_pred_acc_H2_cnn_testset[1]))

model = Sequential()
model.add(Dense(60, input_dim=21, activation='relu'))
model.add(Dense(30, activation = 'relu' ))
model.add(Dense(1, activation='sigmoid'))
	# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_H3, Y_train_H3, epochs=20)

pred_train= model.predict(X_train_H3)
scores = model.evaluate(X_train_H3, Y_train_H3, verbose=0)
print('Accuracy on training data: {}% \n Error on training data: {}'.format(scores[1], 1 - scores[1]))   
 
pred_test= model.predict(X_test_H3)
y_pred_acc_H3_cnn = model.evaluate(X_test_H3, Y_test_H3, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_H3_cnn[1], 1 - y_pred_acc_H3_cnn[1]))  

pred_test= model.predict(H_bin_3_x_testset)
y_pred_acc_H3_cnn_testset = model.evaluate(H_bin_3_x_testset, H_bin_3_y_testset, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_H3_cnn_testset[1], 1 - y_pred_acc_H3_cnn_testset[1]))

model = Sequential()
model.add(Dense(60, input_dim=21, activation='relu'))
model.add(Dense(30, activation = 'relu' ))
model.add(Dense(1, activation='sigmoid'))
	# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_H4, Y_train_H4, epochs=20)

pred_train= model.predict(X_train_H4)
scores = model.evaluate(X_train_H4, Y_train_H4, verbose=0)
print('Accuracy on training data: {}% \n Error on training data: {}'.format(scores[1], 1 - scores[1]))   
 
pred_test= model.predict(X_test_H4)
y_pred_acc_H4_cnn = model.evaluate(X_test_H4, Y_test_H4, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_H4_cnn[1], 1 - y_pred_acc_H4_cnn[1]))  

pred_test= model.predict(H_bin_4_x_testset)
y_pred_acc_H4_cnn_testset = model.evaluate(H_bin_4_x_testset, H_bin_4_y_testset, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_H4_cnn_testset[1], 1 - y_pred_acc_H4_cnn_testset[1]))

model = Sequential()
model.add(Dense(60, input_dim=21, activation='relu'))
model.add(Dense(30, activation = 'relu' ))
model.add(Dense(1, activation='sigmoid'))
	# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_H5, Y_train_H5, epochs=20)

pred_train= model.predict(X_train_H5)
scores = model.evaluate(X_train_H5, Y_train_H5, verbose=0)
print('Accuracy on training data: {}% \n Error on training data: {}'.format(scores[1], 1 - scores[1]))   
 
pred_test= model.predict(X_test_H5)
y_pred_acc_H5_cnn = model.evaluate(X_test_H5, Y_test_H5, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_H5_cnn[1], 1 - y_pred_acc_H5_cnn[1]))  

pred_test= model.predict(H_bin_5_x_testset)
y_pred_acc_H5_cnn_testset = model.evaluate(H_bin_5_x_testset, H_bin_5_y_testset, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_H5_cnn_testset[1], 1 - y_pred_acc_H5_cnn_testset[1]))



model = Sequential()
model.add(Dense(60, input_dim=21, activation='relu'))
model.add(Dense(30, activation = 'relu' ))
model.add(Dense(1, activation='sigmoid'))
	# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_I1, Y_train_I1, epochs=20)

pred_train= model.predict(X_train_I1)
scores = model.evaluate(X_train_I1, Y_train_I1, verbose=0)
print('Accuracy on training data: {}% \n Error on training data: {}'.format(scores[1], 1 - scores[1]))   
 
pred_test= model.predict(X_test_I1)
y_pred_acc_I1_cnn = model.evaluate(X_test_I1, Y_test_I1, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_I1_cnn[1], 1 - y_pred_acc_I1_cnn[1]))  

pred_test= model.predict(I_bin_1_x_testset)
y_pred_acc_I1_cnn_testset = model.evaluate(I_bin_1_x_testset, I_bin_1_y_testset, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_I1_cnn_testset[1], 1 - y_pred_acc_I1_cnn_testset[1]))

model = Sequential()
model.add(Dense(60, input_dim=21, activation='relu'))
model.add(Dense(30, activation = 'relu' ))
model.add(Dense(1, activation='sigmoid'))
	# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_I2, Y_train_I2, epochs=20)

pred_train= model.predict(X_train_I2)
scores = model.evaluate(X_train_I2, Y_train_I2, verbose=0)
print('Accuracy on training data: {}% \n Error on training data: {}'.format(scores[1], 1 - scores[1]))   
 
pred_test= model.predict(X_test_I2)
y_pred_acc_I2_cnn = model.evaluate(X_test_I2, Y_test_I2, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_I2_cnn[1], 1 - y_pred_acc_I2_cnn[1]))  

pred_test= model.predict(I_bin_2_x_testset)
y_pred_acc_I2_cnn_testset = model.evaluate(I_bin_2_x_testset, I_bin_2_y_testset, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_I2_cnn_testset[1], 1 - y_pred_acc_I2_cnn_testset[1]))

model = Sequential()
model.add(Dense(60, input_dim=21, activation='relu'))
model.add(Dense(30, activation = 'relu' ))
model.add(Dense(1, activation='sigmoid'))
	# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_I3, Y_train_I3, epochs=20)

pred_train= model.predict(X_train_I3)
scores = model.evaluate(X_train_I3, Y_train_I3, verbose=0)
print('Accuracy on training data: {}% \n Error on training data: {}'.format(scores[1], 1 - scores[1]))   
 
pred_test= model.predict(X_test_I3)
y_pred_acc_I3_cnn = model.evaluate(X_test_I3, Y_test_I3, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_I3_cnn[1], 1 - y_pred_acc_I3_cnn[1]))  

pred_test= model.predict(I_bin_3_x_testset)
y_pred_acc_I3_cnn_testset = model.evaluate(I_bin_3_x_testset, I_bin_3_y_testset, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_I3_cnn_testset[1], 1 - y_pred_acc_I3_cnn_testset[1]))

model = Sequential()
model.add(Dense(60, input_dim=21, activation='relu'))
model.add(Dense(30, activation = 'relu' ))
model.add(Dense(1, activation='sigmoid'))
	# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_I4, Y_train_I4, epochs=20)

pred_train= model.predict(X_train_I4)
scores = model.evaluate(X_train_I4, Y_train_I4, verbose=0)
print('Accuracy on training data: {}% \n Error on training data: {}'.format(scores[1], 1 - scores[1]))   
 
pred_test= model.predict(X_test_I4)
y_pred_acc_I4_cnn = model.evaluate(X_test_I4, Y_test_I4, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_I4_cnn[1], 1 - y_pred_acc_I4_cnn[1]))

pred_test= model.predict(I_bin_4_x_testset)
y_pred_acc_I4_cnn_testset = model.evaluate(I_bin_4_x_testset, I_bin_4_y_testset, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_I4_cnn_testset[1], 1 - y_pred_acc_I4_cnn_testset[1]))

model = Sequential()
model.add(Dense(60, input_dim=21, activation='relu'))
model.add(Dense(30, activation = 'relu' ))
model.add(Dense(1, activation='sigmoid'))
	# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_I5, Y_train_I5, epochs=20)

pred_train= model.predict(X_train_I5)
scores = model.evaluate(X_train_I5, Y_train_I5, verbose=0)
print('Accuracy on training data: {}% \n Error on training data: {}'.format(scores[1], 1 - scores[1]))   
 
pred_test= model.predict(X_test_I5)
y_pred_acc_I5_cnn = model.evaluate(X_test_I5, Y_test_I5, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_I2_cnn[1], 1 - y_pred_acc_I2_cnn[1]))  

pred_test= model.predict(I_bin_5_x_testset)
y_pred_acc_I5_cnn_testset = model.evaluate(I_bin_5_x_testset, I_bin_5_y_testset, verbose=0)
print('Accuracy on test data: {}% \n Error on test data: {}'.format(y_pred_acc_I5_cnn_testset[1], 1 - y_pred_acc_I5_cnn_testset[1]))



CNN_HospAdmTime = [y_pred_acc_H1_cnn, y_pred_acc_H2_cnn, y_pred_acc_H3_cnn, y_pred_acc_H4_cnn, y_pred_acc_H5_cnn]

CNN_Iculos = [1, y_pred_acc_I2_cnn, y_pred_acc_I3_cnn, y_pred_acc_I4_cnn, y_pred_acc_I5_cnn]

CNN_HospAdmTime_testset = [y_pred_acc_H1_cnn_testset, y_pred_acc_H2_cnn_testset, y_pred_acc_H3_cnn_testset, y_pred_acc_H4_cnn_testset, y_pred_acc_H5_cnn_testset]

CNN_Iculos_testset = [y_pred_acc_I1_cnn_testset, y_pred_acc_I2_cnn_testset, y_pred_acc_I3_cnn_testset, y_pred_acc_I4_cnn_testset, y_pred_acc_I5_cnn_testset]





CNN_Iculos_test = [CNN_Iculos[0], CNN_Iculos[1][1], CNN_Iculos[2][1], CNN_Iculos[3][1], CNN_Iculos[4][1]]

CNN_Iculos = CNN_Iculos_test

CNN_Iculos = pd.DataFrame(CNN_Iculos)

CNN_Iculos.columns = ['CNN_Iculos']

CNN_Iculos







CNN_Iculos = pd.DataFrame(CNN_Iculos)



CNN_HospAdmTime = [CNN_HospAdmTime[0][1], CNN_HospAdmTime[1][1], CNN_HospAdmTime[2][1], CNN_HospAdmTime[3][1], CNN_HospAdmTime[4][1]]

CNN_HospAdmTime = pd.DataFrame(CNN_HospAdmTime)

CNN_HospAdmTime.columns = ['CNN_HospAdmTime']





CNN_HospAdmTime_testset = [CNN_HospAdmTime_testset[0][1], CNN_HospAdmTime_testset[1][1], CNN_HospAdmTime_testset[2][1], CNN_HospAdmTime_testset[3][1], CNN_HospAdmTime_testset[4][1]]

CNN_HospAdmTime_testset = pd.DataFrame(CNN_HospAdmTime_testset)

CNN_HospAdmTime_testset.columns = ['CNN_HospAdmTime_testset']





CNN_Iculos_testset = [CNN_Iculos_testset[0][1], CNN_Iculos_testset[1][1], CNN_Iculos_testset[2][1], CNN_Iculos_testset[3][1], CNN_Iculos_testset[4][1]]

CNN_Iculos_testset = pd.DataFrame(CNN_Iculos_testset)

CNN_Iculos_testset.columns = ['CNN_Iculos_testset']





SVM_Iculos = pd.DataFrame(SVM_Iculos)

SVM_Iculos.columns = ['SVM_Iculos']





SVM_Iculos_testset = pd.DataFrame(SVM_Iculos_testset)

SVM_Iculos_testset.columns = ['SVM_Iculos_testset']



SVM_HospAdmTime = pd.DataFrame(SVM_HospAdmTime)

SVM_HospAdmTime.columns = ['SVM_HospAdmTime']



SVM_HospAdm_testset = pd.DataFrame(SVM_HospAdm_testset)

SVM_HospAdm_testset.columns = ['SVM_HospAdm_testset']





Logreg_Iculos = pd.DataFrame(Logreg_Iculos)

Logreg_Iculos.columns = ['Logreg Iculos']



Logreg_Iculos_testset = pd.DataFrame(Logreg_Iculos_testset)

Logreg_Iculos_testset.columns = ['Logreg_Iculos_testset']



Logreg_HospAdmTime = pd.DataFrame(Logreg_HospAdmTime)

Logreg_HospAdmTime.columns = ['Logreg_HospAdmTime']

Logreg_HospAdmTime_testset = pd.DataFrame(Logreg_HospAdmTime_testset)

Logreg_HospAdmTime_testset.columns = ['Logreg_HospAdmTime_testset']



Naive_bayes_Iculos = pd.DataFrame(Naive_bayes_Iculos)

Naive_bayes_Iculos.columns = ['Naive_bayes_Iculos']



Naive_bayes_Iculos_testset = pd.DataFrame(Naive_bayes_Iculos_testset)

Naive_bayes_Iculos_testset.columns = ['Naive_bayes_Iculos_testset']

Naive_bayes_HospAdmTime = pd.DataFrame(Naive_bayes_HospAdmTime)

Naive_bayes_HospAdmTime.columns = ['Naive_bayes_HospAdmTime']

Naive_bayes_HospAdm_testset = pd.DataFrame(Naive_bayes_HospAdm_testset)

Naive_bayes_HospAdm_testset.columns = ['Naive_bayes_HospAdm_testset']

Naive_bayes_HospAdm_testset



Hosp_df.index = ['Bin_1(0-2 min)', 'Bin_2(2-6 min)', 'Bin_3(6-20 min)', 'Bin_4(20-80min)', 'Bin_5(80min)']

Hosp_df

Iculos_df.index = ['Bin_1(0-4 min)', 'Bin_2(4-8 min)', 'Bin_3(8-13 min)', 'Bin_4(13-21min)', 'Bin_5(21min)']



"""# Conclusions

This dataset provided more predictability than anticpated, with three of the four models performing almost identically. 


*   Naive-bayes was the worst performing model across the board, dropping as low as 60 percent ont the expanded test dataset. 
*   Log-reg, SVM and the Keras Neural Net performed identically for bins 2-4. It was only out performed by the neural net during bin 5. 

This implies (but does not guaruntee) that the dataset itself lends well to the predictability of sepsis. Confirming this notion would require additional testing on data that extends beyond the data contained in the sepsis document, but early analysis shows that all three models found similar trends in the data. The CNN out-performed by a very small margin across the board, but requires notably more compute to get to a similar outcome. Additional information from an organization about the feasibility/cost they would be willing to incur would direct the next steps of an integration.
"""

Hosp_df.plot.line(figsize = (10,10), ylabel= 'Percentage of data points predicted correctly')

Iculos_df.plot.line(figsize = (10,10), ylabel= 'Percentage of data points predicted correctly')











Hosp_df



Hosp_df = Hosp_df.join(CNN_HospAdmTime_testset, how="outer")





Iculos_df

Iculos_df = Iculos_df.join(CNN_Iculos_testset, how="outer")







Random_Df = Naive_bayes_HospAdmTime.join(Naive_bayes_HospAdm_testset, how="outer")

Random_Df = Random_Df.join(CNN_Iculos_testset, how = "outer")



Logreg_HospAdmTime_testset













"""# Further Research
Below are significant considerations that an organization would need to understand prior to utilizing a system like this.  


1.   Cost Analysis: The organization in question would first need to determine how much they would be willing to spend to deploy a tool like the one listed up above. The ideal outcome is a mesh of a properly calculated margin of error and the cost the organization would encur. 
2.   Use Case: We need to understand how a doctor would go about using a platform like this. What about understanding the potential for sepsis risk makes a doctors life easier? Meeting with and understanding where we could place this data could improve the ease of access and the likihood of use.  
3.   Feasibility: If certain varaibles are easier to gather than others we may need to think through reworking our model to handle variability in the data. We may find that certain key values are easy to collect when a patient enters the hospital. This outcome means it would be worth exploring model deployments that can handle the variability in biomarker inputs. 
3.   Margin of Error: Running an analysis against the performance of human diagnoses of sepsis prediction would give us more information about how many additional cases we might catch per year. This margin of error would also entail understanding how many cases our hospitals would be okay with being diagnosed incorrectly. It's likely that type 1 errors result in more drastic outcomes, so meeting with key personelle and ensuring that they understand the limitations of modeling is important. 
4.   Processing Time and Rate of Data Transfer: If we are interacting with IOT devices we may need to inspect the rate at which we can get our data imported, cleaned and ready to be ingested by models. Exploring options like AWS firehose could increase performance and allow us to cache frequently access data closer to the location it's needed. Understanding the data engineering concerns is vital to ensuring the delivery time of our model matches the needs of the organization.  
5.   Data Collection Methods: Understanding variability in data collection methods could allow us to develop methods that automatically adjust/average data coming in about the patient. With blood pressure being as unreliable as it is, we might find that leaning on more concrete data gives us better outputs.

# Works Cited
Burdick, Hoyt et al. "Validation Of A Machine Learning Algorithm For Early Severe Sepsis Prediction: A Retrospective Study Predicting Severe Sepsis Up To 48 H In Advance Using A Diverse Dataset From 461 US Hospitals". BMC Medical Informatics And Decision Making, vol 20, no. 1, 2020. Springer Science And Business Media LLC, doi:10.1186/s12911-020-01284-x.

Kok, Christopher et al. "Automated Prediction Of Sepsis Using Temporal Convolutional Network". Computers In Biology And Medicine, vol 127, 2020, p. 103957. Elsevier BV, doi:10.1016/j.compbiomed.2020.103957.

Paoli, Carly J. et al. "Epidemiology And Costs Of Sepsis In The United States—An Analysis Based On Timing Of Diagnosis And Severity Level*". Critical Care Medicine, vol 46, no. 12, 2018, pp. 1889-1897. Ovid Technologies (Wolters Kluwer Health), doi:10.1097/ccm.0000000000003342.

Rather, Adil Rafiq, and Basharat Kasana. "The Third International Consensus Definitions For Sepsis And Septic Shock (Sepsis-3)". JMS SKIMS, vol 18, no. 2, 2015, pp. 162-164. Sheri-I-Kashmir Institute Of Medical Sciences, doi:10.33883/jms.v18i2.269.

Wong, Andrew et al. "External Validation Of A Widely Implemented Proprietary Sepsis Prediction Model In Hospitalized Patients". JAMA Internal Medicine, 2021. American Medical Association (AMA), doi:10.1001/jamainternmed.2021.2626.
"""